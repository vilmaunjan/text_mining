{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function reads data file\n",
    "def read_text_file(f):\n",
    "    df_complete = pd.read_csv(f)\n",
    "    df = df_complete.loc[:,[\"sentiment\",\"comment\"]] \n",
    "    df.dropna(how=\"any\", inplace=True) #drops columns that are not used\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                            comment\n",
      "0          1  She present class materials with powerpoint wh...\n",
      "1          1  The instructor was generally quite good at exp...\n",
      "2          0  I cant really tell how effective the instructi...\n",
      "3          1  She did a good job of explaining the logic beh...\n",
      "4          1  The activities we did in classed where explain...\n"
     ]
    }
   ],
   "source": [
    "df = read_text_file(\"500/cleaned.csv\")\n",
    "print (df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vilma\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.39 s\n"
     ]
    }
   ],
   "source": [
    "#Train Doc2Vec - considering each comment a document\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "w = re.compile(\"\\w+\",re.I)\n",
    "\n",
    "#Doc2vec only receive labeled sentences so the following method creates a label for each comment\n",
    "def label_sentences(df):\n",
    "    labeled_sentences = []\n",
    "    for index, datapoint in df.iterrows():\n",
    "        tokenized_words = re.findall(w,datapoint[\"comment\"].lower())\n",
    "        labeled_sentences.append(LabeledSentence(words=tokenized_words, tags=['SENT_%s' %index]))\n",
    "    return labeled_sentences\n",
    "\n",
    "\n",
    "def train_doc2vec_model(labeled_sentences):\n",
    "    model = Doc2Vec(min_count=1, window=8, size=100, alpha=0.025, min_alpha=0.025)\n",
    "    \n",
    "    #The following line creates a vocabulary table, digesting all the words and filtering out the unique words, and doing some basic counts on them\n",
    "    model.build_vocab(labeled_sentences)\n",
    "    for epoch in range(10):\n",
    "        #trains Doc2Vec on variable learning rate sequentially decreasing.\n",
    "        model.train(labeled_sentences,total_examples=model.corpus_count, epochs=model.iter)\n",
    "        model.alpha -= 0.002 \n",
    "        model.min_alpha = model.alpha\n",
    "    \n",
    "    return model\n",
    "\n",
    "sen = label_sentences(df)\n",
    "%time model = train_doc2vec_model(sen) #calls to train the model, and gives the time it takes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring a VectorÂ¶ \n",
    "\n",
    "One important thing to note is that you can now infer a vector for any piece of text without having to re-train the model by passing a list of words to the model.infer_vector function. This vector can then be compared with other vectors via cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.55407393e-03,  -1.47841424e-02,  -6.65727956e-03,\n",
       "         1.87597498e-02,   2.86649447e-02,  -2.10007075e-02,\n",
       "         1.91084221e-02,   2.29063965e-02,  -8.31268355e-03,\n",
       "        -1.05696451e-02,   2.13856634e-04,   2.78418362e-02,\n",
       "        -5.16085653e-03,  -2.68914662e-02,   6.06097281e-02,\n",
       "         2.81181969e-02,  -1.31330760e-02,   2.18424015e-02,\n",
       "         1.78850722e-02,  -5.04343696e-02,  -7.78315868e-03,\n",
       "         2.84297112e-02,   1.19949570e-02,   8.95569660e-03,\n",
       "        -2.67488360e-02,  -1.70973979e-03,   2.58434750e-02,\n",
       "         1.17072044e-02,   3.95860150e-02,  -2.06501712e-03,\n",
       "         1.10748690e-02,  -5.20406477e-03,   7.09436983e-02,\n",
       "        -7.98506662e-03,  -1.11838719e-02,   3.27165686e-02,\n",
       "         6.36243029e-03,  -5.16321696e-03,   4.32418510e-02,\n",
       "        -2.49316841e-02,   2.70249862e-02,  -3.07538658e-02,\n",
       "         2.26640031e-02,   2.73975134e-02,  -4.12394218e-02,\n",
       "        -3.85291167e-02,  -4.14932147e-02,   4.14330652e-03,\n",
       "         1.77087775e-03,   4.04779566e-04,  -4.72207479e-02,\n",
       "         2.06966940e-02,   2.52480339e-02,   4.49131802e-02,\n",
       "         7.13138841e-03,  -1.07907625e-02,   5.01312576e-02,\n",
       "        -1.87960751e-02,  -1.81592647e-02,   2.40614591e-03,\n",
       "        -1.46376276e-02,   3.65667790e-02,  -5.52093871e-02,\n",
       "         2.02612653e-02,   1.76478680e-02,   2.75853314e-02,\n",
       "        -1.21186636e-02,  -3.01308166e-02,   1.51278218e-02,\n",
       "        -3.54068428e-02,   1.37026906e-02,   3.47800180e-02,\n",
       "        -1.88280605e-02,  -1.53879542e-02,   1.64671743e-03,\n",
       "         2.26087985e-03,   1.98151525e-02,   8.87072459e-03,\n",
       "        -2.95547415e-02,  -2.65939608e-02,  -5.64111178e-05,\n",
       "        -2.97718169e-03,  -5.01505006e-03,  -1.78614371e-02,\n",
       "        -4.29710262e-02,   1.97061757e-03,  -2.16364507e-02,\n",
       "        -2.00049020e-02,  -2.10602325e-03,  -5.11216512e-03,\n",
       "        -2.86239199e-02,  -2.69886348e-02,  -1.08718742e-02,\n",
       "         2.71564908e-02,  -1.44273927e-02,   1.58915557e-02,\n",
       "         3.13056372e-02,   1.35320406e-02,   1.19761070e-02,\n",
       "         2.52870824e-02], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(['only', 'you', 'can', 'prevent', 'forrest', 'fires'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0159505 , -0.03893923,  0.02340249,  0.01307224,  0.03451093,\n",
       "       -0.00656774, -0.02457786,  0.06272283, -0.0298181 , -0.08020693,\n",
       "       -0.0162821 ,  0.07188087,  0.04145225, -0.15585591,  0.14139356,\n",
       "        0.0873617 , -0.02294507,  0.0879023 , -0.03360162, -0.17169152,\n",
       "       -0.05919168,  0.07944062,  0.05829292,  0.0142904 ,  0.06803995,\n",
       "       -0.02852449,  0.14204425,  0.0545768 ,  0.05104154,  0.00896609,\n",
       "       -0.04531134, -0.07706524,  0.09572662, -0.08720192, -0.01756951,\n",
       "        0.11524361, -0.01322339, -0.0378295 ,  0.01805082,  0.08674865,\n",
       "        0.08608117,  0.04490526,  0.03913374,  0.15469056, -0.05246277,\n",
       "       -0.04330213, -0.09590889, -0.03762541,  0.0629676 ,  0.01090413,\n",
       "       -0.06410495, -0.00426013,  0.0222648 ,  0.07095952, -0.02104527,\n",
       "       -0.02962356,  0.12879348, -0.11376718, -0.12841707,  0.03012384,\n",
       "       -0.06069612,  0.05788385, -0.12934239, -0.01060066, -0.04596388,\n",
       "       -0.01550295, -0.14706556,  0.09882553, -0.02576829, -0.0985335 ,\n",
       "        0.0825719 , -0.00298543, -0.05003292, -0.01393382, -0.01165492,\n",
       "        0.11115348,  0.03742485,  0.03022383, -0.02127462, -0.07885302,\n",
       "        0.02493979,  0.05878007,  0.00521416,  0.00480156, -0.13961464,\n",
       "        0.03886451,  0.00176675, -0.1796343 , -0.08101316,  0.06668086,\n",
       "        0.07371622, -0.1058006 , -0.0551246 ,  0.0319822 ,  0.01574615,\n",
       "       -0.01290725,  0.06726457,  0.08509861, -0.05553719,  0.19304694], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This shows the vector for the first comment labeled as SENT_0\n",
    "model.docvecs['SENT_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hat', 0.8923630118370056),\n",
       " ('teaching', 0.890877366065979),\n",
       " ('keeping', 0.8794277906417847),\n",
       " ('instructors', 0.8748044967651367),\n",
       " ('generally', 0.8627312183380127),\n",
       " ('overall', 0.856669008731842),\n",
       " ('job', 0.8497782945632935),\n",
       " ('jo', 0.8494387865066528),\n",
       " ('logic', 0.8493380546569824),\n",
       " ('reasons', 0.839232325553894)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                            comment  \\\n",
      "0          1  She present class materials with powerpoint wh...   \n",
      "\n",
      "                                 vectorized_comments  \n",
      "0  [0.0159505, -0.0389392, 0.0234025, 0.0130722, ...  \n"
     ]
    }
   ],
   "source": [
    "#The following method stores the vectorized comments in the array comments[]\n",
    "#and stores its label in the array y. This is done so that we can do\n",
    "#the classification using X and y values.\n",
    "def vectorize_comments(df,d2v_model):\n",
    "    y = []\n",
    "    comments = []\n",
    "    for i in range(0,df.shape[0]):\n",
    "        label = 'SENT_%s' %i\n",
    "        comments.append(d2v_model.docvecs[label])\n",
    "    df['vectorized_comments'] = comments\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = vectorize_comments(df,model)\n",
    "print (df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vilma\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Cross validation\n",
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import pickle\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with knn:\n",
    "- doc2vec vectors\n",
    "- k values from 1 to 10\n",
    "- Cross validation fold=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "#This stores a list from 1 to 10, which will be used as k values from 1 to 10\n",
    "k_range=list(range(1,10))\n",
    "print(k_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9]}\n"
     ]
    }
   ],
   "source": [
    "#Creates a dictionary, tuple of the key of nn and its values\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create knn classifier\n",
    "knn=KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use knn classifier, parameters from param_grid, k-fold = 10, and precision as the metrics score\n",
    "grid=GridSearchCV(knn,param_grid,cv=10,scoring='precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert data arrays to lists\n",
    "X=df[\"vectorized_comments\"].T.tolist()\n",
    "y=df[\"sentiment\"].T.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='precision', verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the classifier 'grid' created in line 15 to fit our data\n",
    "#X - represents the comments\n",
    "#y - represents the labels (1 or 0)\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.87862, std: 0.02478, params: {'n_neighbors': 1},\n",
       " mean: 0.89861, std: 0.01473, params: {'n_neighbors': 2},\n",
       " mean: 0.86594, std: 0.02212, params: {'n_neighbors': 3},\n",
       " mean: 0.87992, std: 0.02213, params: {'n_neighbors': 4},\n",
       " mean: 0.85583, std: 0.01840, params: {'n_neighbors': 5},\n",
       " mean: 0.86595, std: 0.02338, params: {'n_neighbors': 6},\n",
       " mean: 0.85371, std: 0.01354, params: {'n_neighbors': 7},\n",
       " mean: 0.86547, std: 0.01970, params: {'n_neighbors': 8},\n",
       " mean: 0.84930, std: 0.01689, params: {'n_neighbors': 9}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the complete results (list of named tuples)\n",
    "#Shows the mean value for each k value, as well as the standard deviation, and its parameters\n",
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 1}\n",
      "[ 0.86956522  0.875       0.85714286  0.89795918  0.89795918  0.86666667\n",
      "  0.83333333  0.87234043  0.88888889  0.92857143]\n",
      "0.878622769453\n"
     ]
    }
   ],
   "source": [
    "# examine the first tuple, when k=1\n",
    "print(grid.grid_scores_[0].parameters)\n",
    "print(grid.grid_scores_[0].cv_validation_scores)\n",
    "print(grid.grid_scores_[0].mean_validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87862276945266327, 0.8986053253891505, 0.86593861770171099, 0.87992042636769996, 0.85582703007190575, 0.86594582319351709, 0.85371127828722515, 0.86546869236432356, 0.84930065113152042]\n"
     ]
    }
   ],
   "source": [
    "# create a list of the mean scores only\n",
    "grid_mean_scores = [result.mean_validation_score for result in grid.grid_scores_]\n",
    "print(grid_mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.898605325389\n",
      "{'n_neighbors': 2}\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
      "           weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "# examine the best model\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Random Forest Classifier (RFC)\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "bootstrap = [True] ---> samples are drawn with replacement if bootstrap=True (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 400 out of 400 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=1, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'min_samples_leaf': [1], 'n_estimators': [200, 400], 'min_samples_split': [2]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='precision', verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'n_estimators': [200,400],\n",
    "    'min_samples_leaf': [1],\n",
    "    'min_samples_split': [2]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(RFC(verbose=1,n_jobs=1), param_grid=parameters, cv=10, scoring='precision')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.87862276945266327, 0.8986053253891505, 0.86593861770171099, 0.87992042636769996, 0.85582703007190575, 0.86594582319351709, 0.85371127828722515, 0.86546869236432356, 0.84930065113152042] Mean of all scores for each cv fold\n"
     ]
    }
   ],
   "source": [
    "# create a list of the mean scores only\n",
    "grid_mean_scores = [result.mean_validation_score for result in grid.grid_scores_]\n",
    "print(grid_mean_scores,\"Mean of all scores for each cv fold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.870728383742\n",
      "{'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=200, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=1, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# examine the best model\n",
    "print(clf.best_score_)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Classifier (SGDC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup 5-fold stratified cross validation\n",
    "cross_v = StratifiedKFold(n_splits=10) #Takes group information into account to avoid building folds with imbalanced class distributions (for binary or multiclass classification tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a parameter grid to search over\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "    'loss': ('log', 'hinge'), #function that gives the amount of error rate\n",
    "    'penalty': ['l1', 'l2', 'elasticnet']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create SGDC classifier using logistic regression and hinge\n",
    "grid_search = GridSearchCV(SGDClassifier(), param_grid=param_grid, cv=cross_v, scoring='precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "       verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'loss': ('log', 'hinge'), 'penalty': ['l1', 'l2', 'elasticnet'], 'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='precision', verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit model to the data\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.87760, std: 0.02873, params: {'loss': 'log', 'penalty': 'l1', 'alpha': 0.0001},\n",
       " mean: 0.85281, std: 0.02104, params: {'loss': 'log', 'penalty': 'l2', 'alpha': 0.0001},\n",
       " mean: 0.85953, std: 0.03016, params: {'loss': 'log', 'penalty': 'elasticnet', 'alpha': 0.0001},\n",
       " mean: 0.88579, std: 0.04406, params: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 0.0001},\n",
       " mean: 0.86118, std: 0.03669, params: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.0001},\n",
       " mean: 0.88110, std: 0.03847, params: {'loss': 'hinge', 'penalty': 'elasticnet', 'alpha': 0.0001},\n",
       " mean: 0.85875, std: 0.03645, params: {'loss': 'log', 'penalty': 'l1', 'alpha': 0.0005},\n",
       " mean: 0.85400, std: 0.02759, params: {'loss': 'log', 'penalty': 'l2', 'alpha': 0.0005},\n",
       " mean: 0.85268, std: 0.01985, params: {'loss': 'log', 'penalty': 'elasticnet', 'alpha': 0.0005},\n",
       " mean: 0.86154, std: 0.03340, params: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 0.0005},\n",
       " mean: 0.86546, std: 0.02766, params: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.0005},\n",
       " mean: 0.85333, std: 0.01834, params: {'loss': 'hinge', 'penalty': 'elasticnet', 'alpha': 0.0005},\n",
       " mean: 0.84450, std: 0.00877, params: {'loss': 'log', 'penalty': 'l1', 'alpha': 0.001},\n",
       " mean: 0.85529, std: 0.02551, params: {'loss': 'log', 'penalty': 'l2', 'alpha': 0.001},\n",
       " mean: 0.85365, std: 0.02365, params: {'loss': 'log', 'penalty': 'elasticnet', 'alpha': 0.001},\n",
       " mean: 0.84484, std: 0.01922, params: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 0.001},\n",
       " mean: 0.85457, std: 0.02238, params: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.001},\n",
       " mean: 0.86635, std: 0.04040, params: {'loss': 'hinge', 'penalty': 'elasticnet', 'alpha': 0.001},\n",
       " mean: 0.83804, std: 0.00677, params: {'loss': 'log', 'penalty': 'l1', 'alpha': 0.005},\n",
       " mean: 0.84573, std: 0.00958, params: {'loss': 'log', 'penalty': 'l2', 'alpha': 0.005},\n",
       " mean: 0.83928, std: 0.00790, params: {'loss': 'log', 'penalty': 'elasticnet', 'alpha': 0.005},\n",
       " mean: 0.83610, std: 0.00454, params: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 0.005},\n",
       " mean: 0.83804, std: 0.00692, params: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.005},\n",
       " mean: 0.83610, std: 0.00454, params: {'loss': 'hinge', 'penalty': 'elasticnet', 'alpha': 0.005},\n",
       " mean: 0.83579, std: 0.00462, params: {'loss': 'log', 'penalty': 'l1', 'alpha': 0.01},\n",
       " mean: 0.83804, std: 0.00692, params: {'loss': 'log', 'penalty': 'l2', 'alpha': 0.01},\n",
       " mean: 0.83863, std: 0.00700, params: {'loss': 'log', 'penalty': 'elasticnet', 'alpha': 0.01},\n",
       " mean: 0.83610, std: 0.00454, params: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 0.01},\n",
       " mean: 0.83551, std: 0.00379, params: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.01},\n",
       " mean: 0.83610, std: 0.00454, params: {'loss': 'hinge', 'penalty': 'elasticnet', 'alpha': 0.01},\n",
       " mean: 0.83610, std: 0.00454, params: {'loss': 'log', 'penalty': 'l1', 'alpha': 0.05},\n",
       " mean: 0.83548, std: 0.00488, params: {'loss': 'log', 'penalty': 'l2', 'alpha': 0.05},\n",
       " mean: 0.83610, std: 0.00454, params: {'loss': 'log', 'penalty': 'elasticnet', 'alpha': 0.05},\n",
       " mean: 0.83610, std: 0.00454, params: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 0.05},\n",
       " mean: 0.83610, std: 0.00454, params: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.05},\n",
       " mean: 0.83610, std: 0.00454, params: {'loss': 'hinge', 'penalty': 'elasticnet', 'alpha': 0.05},\n",
       " mean: 0.83610, std: 0.00454, params: {'loss': 'log', 'penalty': 'l1', 'alpha': 0.1},\n",
       " mean: 0.83579, std: 0.00462, params: {'loss': 'log', 'penalty': 'l2', 'alpha': 0.1},\n",
       " mean: 0.83610, std: 0.00454, params: {'loss': 'log', 'penalty': 'elasticnet', 'alpha': 0.1},\n",
       " mean: 0.83610, std: 0.00454, params: {'loss': 'hinge', 'penalty': 'l1', 'alpha': 0.1},\n",
       " mean: 0.83610, std: 0.00454, params: {'loss': 'hinge', 'penalty': 'l2', 'alpha': 0.1},\n",
       " mean: 0.83610, std: 0.00454, params: {'loss': 'hinge', 'penalty': 'elasticnet', 'alpha': 0.1}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list of the mean scores only\n",
    "grid_search.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.885793439345\n",
      "{'loss': 'hinge', 'penalty': 'l1', 'alpha': 0.0001}\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
      "       penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# examine the best model\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Classifier using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=200))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(np.concatenate(X), np.concatenate(y), test_size=0.02, random_state=17)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=9, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"vectorized_comments\"]\n",
    "df[\"vectorized_comments\"].T.tolist()[0]\n",
    "#y=df[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_pred = classifier.predict(X_test)\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#print(y_test)\n",
    "#print(y_pred)\n",
    "#print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
